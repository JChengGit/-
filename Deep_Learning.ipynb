{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,pymysql\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gensim.models import word2vec\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 100)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文档向量\n",
    "\n",
    "model = word2vec.Word2Vec.load('Zi_vec.bin')\n",
    "\n",
    "# 直接横向相加\n",
    "def wv2dv(t):\n",
    "    text=\"\"\n",
    "    for uchar in t:\n",
    "        if (uchar >= '\\u4e00' and uchar <= '\\u9fa5'):\n",
    "            text+=uchar\n",
    "    lth = len(text)\n",
    "    dv = np.zeros(100)\n",
    "    for i in text:\n",
    "        try:\n",
    "            dv += model.wv[i]\n",
    "        except:\n",
    "            lth-=1\n",
    "    l = max(1,lth)\n",
    "    dv = dv/l\n",
    "    return dv\n",
    "\n",
    "# 矩阵化\n",
    "def wv2matrix(t):\n",
    "    text=\"\"\n",
    "    matrix = []\n",
    "    for uchar in t:\n",
    "        if (uchar >= '\\u4e00' and uchar <= '\\u9fa5'):\n",
    "            text+=uchar\n",
    "    zero = np.zeros(100)\n",
    "    for i in range(80):\n",
    "        try:\n",
    "            matrix.append(model.wv[text[i]])\n",
    "        except:\n",
    "            matrix.append(zero)\n",
    "    return np.array(matrix)\n",
    "\n",
    "# database part\n",
    "CONN = pymysql.connect(host='127.0.0.1', port=3306, user='root', passwd='1234', db='topic')\n",
    "cur = CONN.cursor()\n",
    "\n",
    "def fromdb(topic):\n",
    "    rs=[]\n",
    "    for i in range(1,900):\n",
    "        try:\n",
    "            sql = \"select content from %s where id=%s\"%(topic,i)\n",
    "            cur.execute(sql)\n",
    "            text = cur.fetchall()[0][0]\n",
    "            rs.append(text)\n",
    "        except:\n",
    "            pass\n",
    "    return rs\n",
    "\n",
    "y=wv2matrix('一半残阳下小楼，朱帘斜控软金钩。倚阑无绪不能愁。有个盈盈骑马过，薄妆浅黛亦风流。见人羞涩却回头。')\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 训练集和测试集\n",
    "\n",
    "topic_dic = {\n",
    "    'war':      0,#np.array([1,0,0,0,0,]),\n",
    "    'scene':    1,#np.array([0,1,0,0,0,]),\n",
    "    'farewell': 2,#np.array([0,0,1,0,0,]),\n",
    "    'travel':   3,#np.array([0,0,0,1,0,]),\n",
    "    'love':     4,#np.array([0,0,0,0,1,]),\n",
    "}\n",
    "\n",
    "x_raw = []\n",
    "y_raw = []\n",
    "topics = ['war','scene','farewell','travel','love']\n",
    "for index in range(5):\n",
    "    tag = topic_dic[topics[index]]\n",
    "    at = fromdb(topics[index])\n",
    "    for i in at:\n",
    "        x_raw.append(wv2matrix(i))\n",
    "        y_raw.append(tag)\n",
    "x = np.array(x_raw)\n",
    "y = np.array(y_raw)\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=6) # 存在切分不均的问题\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1765 samples\n",
      "Epoch 1/20\n",
      "1765/1765 [==============================] - 8s 5ms/sample - loss: 1.5175 - accuracy: 0.2935\n",
      "Epoch 2/20\n",
      "1765/1765 [==============================] - 6s 3ms/sample - loss: 1.4201 - accuracy: 0.3365\n",
      "Epoch 3/20\n",
      "1765/1765 [==============================] - 5s 3ms/sample - loss: 1.3721 - accuracy: 0.4034\n",
      "Epoch 4/20\n",
      "1765/1765 [==============================] - 5s 3ms/sample - loss: 1.3242 - accuracy: 0.4051\n",
      "Epoch 5/20\n",
      "1765/1765 [==============================] - 6s 3ms/sample - loss: 1.3238 - accuracy: 0.4079\n",
      "Epoch 6/20\n",
      "1765/1765 [==============================] - 6s 3ms/sample - loss: 1.3185 - accuracy: 0.4238\n",
      "Epoch 7/20\n",
      "1765/1765 [==============================] - 6s 3ms/sample - loss: 1.3361 - accuracy: 0.4550\n",
      "Epoch 8/20\n",
      "1765/1765 [==============================] - 6s 3ms/sample - loss: 1.2740 - accuracy: 0.4499\n",
      "Epoch 9/20\n",
      "1765/1765 [==============================] - 6s 3ms/sample - loss: 1.2132 - accuracy: 0.4765\n",
      "Epoch 10/20\n",
      "1765/1765 [==============================] - 6s 3ms/sample - loss: 1.1713 - accuracy: 0.5020\n",
      "Epoch 11/20\n",
      "1765/1765 [==============================] - 6s 3ms/sample - loss: 1.1689 - accuracy: 0.4884\n",
      "Epoch 12/20\n",
      "1765/1765 [==============================] - 6s 3ms/sample - loss: 1.1166 - accuracy: 0.4946\n",
      "Epoch 13/20\n",
      "1765/1765 [==============================] - 6s 3ms/sample - loss: 1.0764 - accuracy: 0.5416\n",
      "Epoch 14/20\n",
      "1765/1765 [==============================] - 6s 3ms/sample - loss: 1.0497 - accuracy: 0.5671\n",
      "Epoch 15/20\n",
      "1765/1765 [==============================] - 6s 3ms/sample - loss: 1.0616 - accuracy: 0.5263\n",
      "Epoch 16/20\n",
      "1765/1765 [==============================] - 6s 3ms/sample - loss: 1.0180 - accuracy: 0.5399\n",
      "Epoch 17/20\n",
      "1765/1765 [==============================] - 6s 3ms/sample - loss: 0.9588 - accuracy: 0.5722\n",
      "Epoch 18/20\n",
      "1765/1765 [==============================] - 6s 3ms/sample - loss: 0.9284 - accuracy: 0.6051\n",
      "Epoch 19/20\n",
      "1765/1765 [==============================] - 6s 3ms/sample - loss: 1.0213 - accuracy: 0.5609\n",
      "Epoch 20/20\n",
      "1765/1765 [==============================] - 6s 3ms/sample - loss: 0.9477 - accuracy: 0.5807\n",
      "589/589 - 1s - loss: 1.2412 - accuracy: 0.5059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2411737877160867, 0.5059423]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = tf.keras.models.Sequential([\n",
    "    #tf.keras.layers.Conv1D(filters=50, kernel_size=5, strides=1, padding='valid'),\n",
    "    #tf.keras.layers.MaxPool1D(2, padding='valid'),\n",
    "    #tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.LSTM(100, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(100, return_sequences=True),\n",
    "    #tf.keras.layers.Flatten(input_shape=[80,100]),\n",
    "    #tf.keras.layers.Dense(300, activation='relu'),\n",
    "    #tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    #tf.keras.layers.Dense(5, activation='softmax')\n",
    "    tf.keras.layers.LSTM(5, activation='softmax', return_sequences=False)\n",
    "])\n",
    "\n",
    "model1.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer = \"adam\",\n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "model1.fit(x_train, y_train, epochs=20)\n",
    "model1.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1765 samples\n",
      "Epoch 1/20\n",
      "1765/1765 [==============================] - 1s 507us/sample - loss: 1.6838 - accuracy: 0.3280\n",
      "Epoch 2/20\n",
      "1765/1765 [==============================] - 1s 373us/sample - loss: 1.0412 - accuracy: 0.5745\n",
      "Epoch 3/20\n",
      "1765/1765 [==============================] - 1s 385us/sample - loss: 0.6468 - accuracy: 0.7671\n",
      "Epoch 4/20\n",
      "1765/1765 [==============================] - 1s 351us/sample - loss: 0.3522 - accuracy: 0.9065\n",
      "Epoch 5/20\n",
      "1765/1765 [==============================] - 1s 355us/sample - loss: 0.2534 - accuracy: 0.9337\n",
      "Epoch 6/20\n",
      "1765/1765 [==============================] - 1s 355us/sample - loss: 0.2266 - accuracy: 0.9473\n",
      "Epoch 7/20\n",
      "1765/1765 [==============================] - 1s 347us/sample - loss: 0.2027 - accuracy: 0.9467\n",
      "Epoch 8/20\n",
      "1765/1765 [==============================] - 1s 387us/sample - loss: 0.1784 - accuracy: 0.9479\n",
      "Epoch 9/20\n",
      "1765/1765 [==============================] - 1s 345us/sample - loss: 0.1699 - accuracy: 0.9530\n",
      "Epoch 10/20\n",
      "1765/1765 [==============================] - 1s 361us/sample - loss: 0.1538 - accuracy: 0.9473\n",
      "Epoch 11/20\n",
      "1765/1765 [==============================] - 1s 352us/sample - loss: 0.1262 - accuracy: 0.9524\n",
      "Epoch 12/20\n",
      "1765/1765 [==============================] - 1s 348us/sample - loss: 0.1239 - accuracy: 0.9501\n",
      "Epoch 13/20\n",
      "1765/1765 [==============================] - 1s 342us/sample - loss: 0.1254 - accuracy: 0.9479\n",
      "Epoch 14/20\n",
      "1765/1765 [==============================] - 1s 350us/sample - loss: 0.1226 - accuracy: 0.9484\n",
      "Epoch 15/20\n",
      "1765/1765 [==============================] - 1s 389us/sample - loss: 0.1180 - accuracy: 0.9490\n",
      "Epoch 16/20\n",
      "1765/1765 [==============================] - 1s 351us/sample - loss: 0.1111 - accuracy: 0.9501\n",
      "Epoch 17/20\n",
      "1765/1765 [==============================] - 1s 352us/sample - loss: 0.1055 - accuracy: 0.9524\n",
      "Epoch 18/20\n",
      "1765/1765 [==============================] - 1s 365us/sample - loss: 0.1003 - accuracy: 0.9456\n",
      "Epoch 19/20\n",
      "1765/1765 [==============================] - 1s 349us/sample - loss: 0.0923 - accuracy: 0.9484\n",
      "Epoch 20/20\n",
      "1765/1765 [==============================] - 1s 400us/sample - loss: 0.0930 - accuracy: 0.9501\n",
      "589/589 - 0s - loss: 2.0188 - accuracy: 0.5161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0188183833059106, 0.516129]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=50, kernel_size=5, strides=1, padding='valid'),\n",
    "    tf.keras.layers.MaxPool1D(2, padding='valid'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    #tf.keras.layers.Flatten(input_shape=[80,100]),\n",
    "    #tf.keras.layers.Dense(300, activation='relu'),\n",
    "    #tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model2.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer = \"adam\",\n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "model2.fit(x_train, y_train, epochs=20)\n",
    "model2.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
